# Data Engineering

From the outset, a large number of issues were present just in the gathering and assembling of the data.  The most immediate problem that needed to be solved was related to the size and scale of the data that would need to be available in a manner that members of the team could access remotely and ensure.  While cleaning and creating connections to structure and normalize the data, the team needed to be able to ensure that work was not siloed, creating a risk that the data would be inconsistent and disjointed.  The hundreds of gigabytes of data would exceed the limits enforced by GitHub.  GitHub recommends a single file limit of 1MB and enforces the limit at 100MB.  While GitHub does offer a large file storage option, it was not going to suit the needs of this project. We needed to 100GB of compressed files or nearly 300GB of uncompressed file.  Professor Jed Rembold was able to suggest a novel solution, which was ultimately pursued to remedy this issue.  He suggested the use of an Amazon Web Services (AWS) S3 (Simple Storage Service) bucket to store the data, or one of many other cloud storage options, such as Microsoft Azure Blob Storage or Google Cloud Storage.  Amazon Web Services was selected after researching the various options.  The AWS S3 bucket that our team created allowed for the data to be stored in digital buckets, making all files accessible anywhere at any time via the cloud.  This provided a safe place to house any large new files as the data was explored and connected.  Access could be standardized and integrated into exploration.  Setting up and establishing the bucket was no small feat and required extensive investigation and practice.  Neither member of our team had leveraged AWS in any form prior to this project.  It became immediately apparent that this solution could be very complex and difficult for novices to approach.  Fortunately, LLM support from Copilot and Chat GPT was effective in working through the various technical complexities and making this solution more approachable.  Furthermore, AWS has an integrated LLM which provides further support to setting up the S3, facilitating access to both team members, and establishing tokens for remote access.

Setting up the AWS S3 was not the only task in this solution.  Professor Rembold also recommended leveraging DuckDB to manage the database.  DuckDB is an in-process SQL OLAP (Online Analytical Processing) database management system.  It is specifically designed for fast analytical queries on large datasets.  It also does not require a server to run.  Yet another piece of the puzzle was Professor Remboldâ€™s third piece of advice, which was to leverage the Parquet format rather than the CSV and text file formats that were ingested from CMS, the FDA, and the US Census Bureau.  Parquet is a columnar storage file format that was designed to enhance data processing efficiency.  It is leveraged in big data and analytics environments.  It works natively with DuckDB.  Its efficient compression allowed for all of the files comprising the dataset to be stored in the same bucket and be worked on using similar scripting and access processes.

This recommended structure proved to be a profound solution to manage the database.  The AWS S3 bucket, DuckDB, and Parquet file formats all worked together and could be libraried in R to allow for reusable scripts that could be stored in the GitHub repository.  This allowed the team to work asynchronously on the same data, which was still connected to the same repository.  This also provided an excellent opportunity to expand and explore some of the additional features provided by RStudio, specifically with GitHub integration.

Over 100 different Parquet files were stored in the S3 bucket.  Many of these were variations of the plan files, geographic locator files, drug formulary files, and drug pricing files from the CMS quarterly formulary data set.  It was not expected that all files would be used, but the early exploration necessitated the availability of every quarter from every year (2019-2023) of the data set.  Ultimately, the Census, NDC, and CMS spending data sets were much more reasonable from a size perspective, and only using those would not have necessitated the use of a large, scalable storage option.  Using the S3 made the process of engineering the data more complicated, but it was the right solution for this project.  It made the process of structuring a relational database with all of the desired files achievable.

Our team took an ELT (Extract, Load, Transform) approach to integrating the data and preparing it for analysis.  Analyzing and creating database connections would take place after it had been extracted from the various aforementioned sources, loaded into our S3 bucket.  The S3 bucket effectively served the role of data warehouse.  Both team members could then remotely connect to the remote cloud warehouse by way of scripts in R, constructed around SQL queries inside of DuckDB functions to query information.  Those scripts could be stored and saved in the GitHub repository.  This allowed the team to scale and build upon insights, solutions, and code.  This facilitated a harmonious mechanism for analysis, discovery, and insights.

The process of connecting all the files was not without challenges, many of which were mentioned and addressed during data collection.  One of the larger clearing and connecting issues was related to NDC formats.  The NDC number is typically in an 11-digit format divided into three segments.  However, there are other common formats and structures that may or may not include page details.  There are also HIPAA standard formats, which add a leading zero.  The NDC files utilized an 11-digit product NDC code with hyphens separating the three sections: labeler code (5 digits), product code (4 digits), and the package code (3 digits).  The CMS formulary files utilized a 9-digit NDC code without a hyphen (only the labeler and product codes).  It took a bit of trial and experimentation to transform the files and get them to cooperate with each other.  Another critical hurdle was connecting the NDC files to the CMS spending data, which required string matching on generic medication names.  Fortunately, the majority of the medications were able to match product NDC codes, but were limited by a lack of package details.  While medication names are unique, the string structure can have simple variations, which creates a significant risk of not being able to match a great number of the medications our team hoped to analyze.

Upon completion of the preliminary engineering of the data for this project, our team could effectively connect insights and data from the census at the county level, link that to CMS formulary pricing under Medicare Part D, connect that to NDC codes, and lastly connect that to CMS spending details.  The data was ready for analysis and further refinement.  With the data connected, it could be condensed and aggregated in a manner that allowed for exploration of medication pricing and its impact across various demographics within the U.S.
