# Data

## Data Collection

### Data Sources

We selected the CMS Quarterly Prescription Drug Plan Formulary as our primary data source, utilizing pricing, formulary, plan information, and geographic locator files to map drug prices and plan availability at the county level. The formulary offered a rare combination of granularity, geographic coverage, and accessibility, making it a practical foundation for our analysis.

Demographic data selection was straightforward. U.S. Census county-level data was collected using the Tidycensus [@tidycensus2025] package in R. Key variables included age, education, gender, race, total population, median income, and poverty rates.

To interpret drug references in the CMS datasets, which list medications by National Drug Code (NDC) only, we added the FDA’s NDC Directory [@fda2025ndc] to match codes with drug names.

In addition to the formulary, we integrated three national-level CMS datasets (Medicare Part B, Part D, and Medicaid, 2019–2023) to contextualize drug spending trends. Although lacking county-level detail, these datasets highlighted medications with the highest national financial impact.

### Data Cleaning and Technical Challenges

Although the previous section may suggest that combining and storing the data was straightforward, the data cleaning and processing phase presented a significant challenge, consistent with the complexities typical of most data science projects.

The CMS Quarterly Prescription Drug Plan Formulary datasets were significantly larger than those our team had previously worked with. Each quarterly file from 2019 Q1 through 2025 Q1 contained approximately 10–12 GB of compressed data. To manage scope, we narrowed our focus to the 2023 Q4 dataset. Even within this single quarter, the formulary included pricing data for approximately 6,336 unique medications (identified by NDC codes), across multiple dosage strengths and 30-, 60-, and 90-day supply options. These were distributed across 5,644 unique contract/plan combinations.
Some Medicare plan options were listed at the county level, while others were available only at the broader regional level, either as Medicare Advantage (MA, also known as Part C) or Prescription Drug Plans (PDP). To align these regional plans with our census-based demographic data, we disaggregated them by county. Medication prices varied not only by NDC code, but also by supply duration, dosage strength (e.g., 10 mg, 30 mg, 50 mg daily), delivery mechanism (e.g., pill, patch, injection, liquid), and Medicare plan.

A key limitation of the CMS formulary data was the absence of drug names, listing only 11-digit National Drug Code (NDC) identifiers. Meanwhile, the overall spending datasets included drug names but lacked NDCs, preventing direct linkage. Since NDCs are not easily interpretable, converting them to readable names was essential for analysis.

To bridge this gap, we used the FDA’s National Drug Code Directory [@fda2025ndc], which provides detailed drug identification. However, formatting inconsistencies, such as missing leading zeros and compressed segments, required standardizing both datasets to a 9-digit format by removing package-level detail and adjusting segment lengths.

After extensive cleaning, we successfully matched FDA drug names to CMS formulary entries. Fortunately, naming conventions in the spending data aligned closely with FDA standards (95% match), enabling us to relate national spending figures to county-level prices and plan availability. This integration created a robust, interconnected database across multiple large-scale datasets.

During the geographic analysis, we identified a unique complication related to Connecticut. In 2022, all Connecticut counties were renamed and had their boundaries redrawn [@countiesConnecticut]. While our primary focus was on Q4 2023 data, a discrepancy emerged: CMS data continued to use the pre-2022 county names and boundaries, whereas the U.S. Census Bureau adopted the updated designations. Because the county shapes also changed, we applied engineering judgment to map the old county definitions to the new ones. As a result, any county-level analysis for Connecticut should be interpreted as an approximation. Data for all other U.S. counties remains accurate once spellings were matched.

## Data Engineering

The project faced substantial challenges in gathering and assembling the data, chief among them, the scale and accessibility of the files. The volume of data, which exceeded 100 GB compressed (nearly 300 GB uncompressed), far surpassed the file size limitations imposed by GitHub. While GitHub does offer a Large File Storage (LFS) extension, it was not sufficient for the scope of this project. Moreover, the team needed a collaborative environment where all members could access, modify, and validate shared datasets without working in silos, which could risk data inconsistency.

To address this, and upon the recommendation of Professor Jed Rembold, we explored several cloud storage options. After evaluating the alternatives, Amazon Web Services (AWS) S3 (Simple Storage Service) was selected for its flexibility, scalability, and broad support within the data science community. Using an S3 bucket, we created a centralized, cloud-based storage system that allowed team members to upload, retrieve, and organize large datasets seamlessly.

Implementing this solution was not trivial. Neither team member had prior experience with AWS, and the learning curve was significant. Support from large language models (LLMs) such as the AWS integrated LLM assistant, Microsoft Copilot, GitHub Copilot and ChatGPT proved instrumental in navigating technical setup, access permissions, and authentication. The result was a robust, scalable storage infrastructure that underpinned our data pipeline and enabled consistent access and collaboration throughout the project.

Setting up the AWS S3 bucket was only the first step in implementing a robust data infrastructure. Professor Rembold also recommended using DuckDB, an in-process SQL OLAP (Online Analytical Processing) database designed for high-performance analytical queries on large datasets. Unlike traditional database systems, DuckDB operates without a dedicated server, making it ideal for decentralized, collaborative data science workflows.

A third critical recommendation was to convert all raw data, originally provided in CSV and text formats by CMS, the FDA, and the U.S. Census Bureau, into Parquet.  Parquet is a columnar storage format optimized for analytical workloads. Parquet offers efficient compression and schema evolution, and it integrates seamlessly with DuckDB. By adopting Parquet, we reduced storage costs and improved query speed, allowing us to manage and process the dataset more effectively within the AWS S3 environment.

Together, AWS S3, DuckDB, and Parquet formed a cohesive, scalable solution. The team was able to interface with this infrastructure using R, enabling the creation of reusable scripts that could be stored and version-controlled in GitHub. This structure supported asynchronous collaboration, ensured data consistency, and also allowed us to explore advanced features of RStudio, particularly its GitHub integration.

Over 100 Parquet files were ultimately stored in the AWS S3 bucket, many representing variations of plan files, geographic locator files, drug formulary files, and pricing data from the CMS Quarterly Prescription Drug Plan Formulary dataset. While not all files were used in the final analysis, early-stage exploration required access to every quarter from 2019 to 2023. In contrast, the U.S. Census, FDA NDC, and CMS spending datasets were far smaller and could have been managed locally. However, the CMS formulary data’s size and complexity made scalable storage essential. Though it introduced some engineering challenges, using S3 was ultimately the right choice. It  enabled the creation of a structured, relational database across a large, disparate dataset.

We implemented an ELT (Extract, Load, Transform) pipeline, extracting data from source systems and loading it into an AWS S3 bucket as a centralized warehouse. Transformations and analysis were performed using R scripts and DuckDB’s SQL interface on Parquet files. Version control via GitHub enabled collaborative, reproducible workflows and scalable, iterative analysis.

Standardizing NDC formats across CMS and FDA datasets allowed consistent linkage of drug identifiers, enabling reliable mapping between county-level pricing, national spending data, and drug names. By the end of the data engineering phase, we had built a fully connected architecture that linked U.S. Census demographics to Medicare Part D pricing, mapped those prices to NDC codes, and connected them to national drug spending. This relational framework supported robust geographic and demographic analysis of medication costs across the U.S.
