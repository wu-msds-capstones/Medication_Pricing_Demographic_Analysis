# Data Engineering

From the outset, the project faced substantial challenges in gathering and assembling the data, chief among them, the scale and accessibility of the files. The volume of data, which exceeded 100 GB compressed (nearly 300 GB uncompressed), far surpassed the file size limitations imposed by GitHub. While GitHub does offer a Large File Storage (LFS) extension, it was not sufficient for the scope of this project. Moreover, the team needed a collaborative environment where all members could access, modify, and validate shared datasets without working in silos, which could risk data inconsistency.

To address this, and upon the recommendation of Professor Jed Rembold, we explored several cloud storage options, including Microsoft Azure Blob Storage, Google Cloud Storage, and Amazon Web Services (AWS) S3 (Simple Storage Service). After evaluating the alternatives, AWS S3 was selected for its flexibility, scalability, and broad support within the data science community. Using S3, we created a centralized, cloud-based storage system that allowed team members to upload, retrieve, and organize large datasets seamlessly.

Implementing this solution was not trivial. Neither team member had prior experience with AWS, and the learning curve was significant. However, support from large language models (LLMs) such as GitHub Copilot and ChatGPT proved instrumental in navigating technical setup, access permissions, and authentication token generation. Additionally, AWS’s built-in LLM support further streamlined the configuration and administration process. The result was a robust, scalable storage infrastructure that underpinned our data pipeline and enabled consistent access and collaboration throughout the project.

Setting up the AWS S3 bucket was only the first step in implementing a robust data infrastructure. Professor Rembold also recommended using DuckDB, an in-process SQL OLAP (Online Analytical Processing) database designed for high-performance analytical queries on large datasets. Unlike traditional database systems, DuckDB operates without a dedicated server, making it ideal for decentralized, collaborative data science workflows.
A third critical recommendation was to convert all raw data, originally provided in CSV and text formats by CMS, the FDA, and the U.S. Census Bureau, into Parquet, a columnar storage format optimized for analytical workloads. Parquet offers efficient compression and schema evolution, and it integrates seamlessly with DuckDB. By adopting Parquet, we reduced storage costs and improved query speed, allowing us to manage and process the dataset more effectively within the AWS S3 environment.

Together, AWS S3, DuckDB, and Parquet formed a cohesive, scalable solution. The team was able to interface with this infrastructure using R, enabling the creation of reusable scripts that could be stored and version-controlled in GitHub. This structure supported asynchronous collaboration, ensured data consistency, and also allowed us to explore advanced features of RStudio, particularly its GitHub integration.
Over 100 Parquet files were ultimately stored in the AWS S3 bucket, many representing variations of plan files, geographic locator files, drug formulary files, and pricing data from the CMS Quarterly Prescription Drug Plan Formulary dataset. While not all files were used in the final analysis, early-stage exploration required access to every quarter from 2019 to 2023. In contrast, the U.S. Census, FDA NDC, and CMS spending datasets were far smaller and could have been managed locally. However, the CMS formulary data’s size and complexity made scalable storage essential. Though it introduced some engineering challenges, using S3 was ultimately the right choice—it enabled the creation of a structured, relational database across a large, disparate dataset.

We adopted an ELT (Extract, Load, Transform) approach. Data was first extracted from the original sources and loaded into the S3 bucket, which functioned as a centralized data warehouse. From there, transformation and analysis occurred via scripts written in R, using DuckDB’s SQL-based query functions to interface with the Parquet files. These scripts were version-controlled in GitHub, allowing both team members to collaborate asynchronously while maintaining consistency and reproducibility. This architecture enabled scalable, iterative analysis and fostered a cohesive workflow for generating insights.

Thanks to the data cleaning process, which standardized NDC formats across CMS and FDA datasets, we were able to join medication identifiers consistently across data sources. This enabled reliable mapping between county-level pricing data, national spending records, and drug names—critical for conducting meaningful demographic and geographic analysis.
By the end of the data engineering phase, our team had established a fully connected data architecture. We could now relate county-level U.S. Census demographic data to CMS Medicare Part D formulary pricing, map those prices to specific NDC codes, and link them to national spending data from CMS. This structured, relational framework enabled aggregation, comparison, and exploration of how medication pricing impacts different demographic groups across the U.S.
